## 所有机器配置 主要配置ip和hostname
## All the servers
[all_servers]
10.150.31.29 host_ip=10.150.31.29 host_name=host-10-150-31-29
#10.150.31.30 host_ip=10.150.31.30 host_name=host-10-150-31-30
10.150.31.31 host_ip=10.150.31.31 host_name=host-10-150-31-31
10.150.31.32 host_ip=10.150.31.32 host_name=host-10-150-31-32
10.150.31.33 host_ip=10.150.31.33 host_name=host-10-150-31-33

## TiDB 集群相关配置
## TiDB Cluster Part
[tidb_servers]
tidb1 ansible_host=10.150.31.32 deploy_dir=/opt/bigdata/tidb
tidb2 ansible_host=10.150.31.33 deploy_dir=/opt/bigdata/tidb

[tikv_servers]
tikv1 ansible_host=10.150.31.29 deploy_dir=/opt/bigdata/tikv
tikv2 ansible_host=10.150.31.31 deploy_dir=/opt/bigdata/tikv
tikv3 ansible_host=10.150.31.32 deploy_dir=/opt/bigdata/tikv

#tikv1-1 ansible_host=172.28.30.28 deploy_dir=/data1/bigdata/tikv tikv_port=20171 tikv_status_port=20181 labels="host=tikv1-1"
#tikv1-2 ansible_host=172.28.30.28 deploy_dir=/data2/bigdata/tikv tikv_port=20172 tikv_status_port=20182 labels="host=tikv1-2"
#tikv2-1 ansible_host=172.28.30.29 deploy_dir=/data1/bigdata/tikv tikv_port=20171 tikv_status_port=20181 labels="host=tikv2-1"
#tikv2-2 ansible_host=172.28.30.29 deploy_dir=/data2/bigdata/tikv tikv_port=20172 tikv_status_port=20182 labels="host=tikv2-2"
#tikv3-1 ansible_host=172.28.30.30 deploy_dir=/data1/bigdata/tikv tikv_port=20171 tikv_status_port=20181 labels="host=tikv3-1"
#tikv3-2 ansible_host=172.28.30.30 deploy_dir=/data2/bigdata/tikv tikv_port=20172 tikv_status_port=20182 labels="host=tikv3-2"

[pd_servers]
tipd1 ansible_host=10.150.31.31 deploy_dir=/opt/bigdata/tipd
tipd2 ansible_host=10.150.31.32 deploy_dir=/opt/bigdata/tipd
tipd3 ansible_host=10.150.31.33 deploy_dir=/opt/bigdata/tipd

[spark_master]

[spark_slaves]

[lightning_server]

[importer_server]

## 大数据集群相关配置
## BigData Cluster Part
[hadoop_servers]
#hadoop1 ansible_host=192.167.8.171 name=master deploy_dir=/opt/bigdata/hadoop
#hadoop2 ansible_host=192.167.8.172 name=slave deploy_dir=/opt/bigdata/hadoop
#hadoop3 ansible_host=192.167.8.173 deploy_dir=/opt/bigdata/hadoop

# hdfs
[namenode_servers]
#192.167.8.171 name=master
#192.167.8.172 name=slave

[datanode_servers]
#192.167.8.171
#192.167.8.172
#192.167.8.173

# yarn
[resourcemanager_servers]
#192.167.8.171
#192.167.8.172

[nodemanager_servers]
#192.167.8.171
#192.167.8.172
#192.167.8.173

[hive_servers]
#hive1 ansible_host=192.167.8.171 deploy_dir=/opt/bigdata/hive

[spark_servers]
#spark1 ansible_host=192.167.8.171 deploy_dir=/opt/bigdata/spark

[presto_servers]
#presto1 ansible_host=10.150.31.29 nodeid=p1 name=coordinator deploy_dir=/opt/bigdata/presto
#presto2 ansible_host=10.150.31.30 nodeid=p2 name=worker deploy_dir=/opt/bigdata/presto
#presto3 ansible_host=10.150.31.31 nodeid=p3 name=worker deploy_dir=/opt/bigdata/presto

[sqoop_servers]
#sqoop1 ansible_host=192.167.8.171 deploy_dir=/opt/bigdata/sqoop

[zookeeper_servers]
#zookeeper1 ansible_host=192.167.8.171 myid=1 deploy_dir=/opt/bigdata/zookeeper
#zookeeper2 ansible_host=192.167.8.172 myid=2 deploy_dir=/opt/bigdata/zookeeper
#zookeeper3 ansible_host=192.167.8.173 myid=3 deploy_dir=/opt/bigdata/zookeeper

## Ds Cluster Part
[ds_servers]
#192.167.8.171 deploy_dir=/opt/bigdata/ds

[ds_api_log_alert]
#ds1 ansible_host=192.167.8.171

[ds_masters]
#ds_master1 ansible_host=192.167.8.171

[ds_workers]
#ds_worker1 ansible_host=192.167.8.171

[mysql_servers]
mysql1 ansible_host=10.150.31.29 deploy_dir=/opt/bigdata/mysql

[redis_servers]
redis1 ansible_host=10.150.31.31 name=master sentinel=true deploy_dir=/opt/bigdata/redis
redis2 ansible_host=10.150.31.32 name=slave sentinel=true deploy_dir=/opt/bigdata/redis
redis3 ansible_host=10.150.31.33 name=slave sentinel=true deploy_dir=/opt/bigdata/redis

[redis_monitored_servers]
#10.150.31.31 deploy_dir=/opt/bigdata/monitoring
#10.150.31.32 deploy_dir=/opt/bigdata/monitoring
#10.150.31.33 deploy_dir=/opt/bigdata/monitoring

[elasticsearch_servers]
elasticsearch1 ansible_host=10.150.31.31 deploy_dir=/opt/bigdata/elasticsearch
elasticsearch2 ansible_host=10.150.31.32 deploy_dir=/opt/bigdata/elasticsearch
elasticsearch3 ansible_host=10.150.31.33 deploy_dir=/opt/bigdata/elasticsearch


## Monitoring Part
# prometheus and pushgateway servers
[monitoring_servers]
10.150.31.29 deploy_dir=/opt/bigdata/monitoring

[grafana_servers]
10.150.31.29 deploy_dir=/opt/bigdata/monitoring

# node_exporter and blackbox_exporter servers
[monitored_servers]
10.150.31.29 deploy_dir=/opt/bigdata/monitoring
#10.150.31.30 deploy_dir=/opt/bigdata/monitoring
10.150.31.31 deploy_dir=/opt/bigdata/monitoring
10.150.31.32 deploy_dir=/opt/bigdata/monitoring
10.150.31.33 deploy_dir=/opt/bigdata/monitoring

[alertmanager_servers]
10.150.31.29 deploy_dir=/opt/bigdata/monitoring

[kafka_exporter_servers]

## Binlog Part
[pump_servers]

[drainer_servers]

## Group variables
[pd_servers:vars]
# location_labels = ["zone","rack","host"]

## Global variables
[all:vars]
deploy_dir = /opt/bigdata
root_dir = /opt/bigdata

## Connection
# ssh via normal user and port
ansible_user = zhujinlong191211
ansible_port = 22

cluster_name = mycluster

tidb_version = v3.0.15

# don't make him special
haproxy_version = 1.5.18

java_version = 1.8.0_261
scala_version = 2.12.12

#zookeeper_version = 3.4.14
zookeeper_version = 3.6.2
hadoop_version = 2.7.7
hive_version = 2.3.7
spark_version = 2.4.4
presto_version = '0.240'
sqoop_version = 1.4.6

ds_version = 1.3.1

mysql_version = 5.7.28
redis_version = 3.2.5

elasticsearch_version = 2.4.1

# process supervision, [systemd, supervise]
process_supervision = systemd

timezone = Asia/Shanghai

enable_firewalld = False
# check NTP service
enable_ntpd = True
set_hostname = False

## binlog trigger
enable_binlog = False

# kafka cluster address for monitoring, example:
# kafka_addrs = "192.168.0.11:9092,192.168.0.12:9092,192.168.0.13:9092"
kafka_addrs = ""

# zookeeper address of kafka cluster for monitoring, example:
# zookeeper_addrs = "192.168.0.11:2181,192.168.0.12:2181,192.168.0.13:2181"
zookeeper_addrs = ""

# enable TLS authentication in the TiDB cluster
enable_tls = False

# KV mode
deploy_without_tidb = False

# wait for region replication complete before start tidb-server.
wait_replication = True

# Optional: Set if you already have a alertmanager server.
# Format: alertmanager_host:alertmanager_port
alertmanager_target = ""

grafana_admin_user = "admin"
grafana_admin_password = "admin"


### Collect diagnosis
collect_log_recent_hours = 2

enable_bandwidth_limit = True
# default: 10Mb/s, unit: Kbit/s
collect_bandwidth_limit = 10000
