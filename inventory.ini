## All the servers
[all_servers]
172.28.30.28
172.28.30.29
172.28.30.30

## TiDB Cluster Part
[tidb_servers]
tidb1 ansible_host=172.28.30.28 deploy_dir=/data1/bigdata/tidb
tidb2 ansible_host=172.28.30.29 deploy_dir=/data1/bigdata/tidb
tidb3 ansible_host=172.28.30.30 deploy_dir=/data1/bigdata/tidb

[tikv_servers]
tikv1-1 ansible_host=172.28.30.28 deploy_dir=/data1/bigdata/tikv tikv_port=20171 tikv_status_port=20181 labels="host=tikv1-1"
#tikv1-2 ansible_host=10.16.40.154 deploy_dir=/data2/bigdata/tikv tikv_port=20172 tikv_status_port=20182 labels="host=tikv1-2"
tikv2-1 ansible_host=172.28.30.29 deploy_dir=/data1/bigdata/tikv tikv_port=20171 tikv_status_port=20181 labels="host=tikv2-1"
#tikv2-2 ansible_host=10.16.40.155 deploy_dir=/data2/bigdata/tikv tikv_port=20172 tikv_status_port=20182 labels="host=tikv2-2"
tikv3-1 ansible_host=172.28.30.30 deploy_dir=/data1/bigdata/tikv tikv_port=20171 tikv_status_port=20181 labels="host=tikv3-1"
#tikv3-2 ansible_host=10.16.40.156 deploy_dir=/data2/bigdata/tikv tikv_port=20172 tikv_status_port=20182 labels="host=tikv3-2"

[pd_servers]
tipd1 ansible_host=172.28.30.28
tipd2 ansible_host=172.28.30.29
tipd3 ansible_host=172.28.30.30

[spark_master]

[spark_slaves]

[lightning_server]

[importer_server]

## BigData Cluster Part
[hadoop_servers]
hadoop1 ansible_host=172.28.30.28 deploy_dir=/data1/bigdata/hadoop
hadoop2 ansible_host=172.28.30.29 deploy_dir=/data1/bigdata/hadoop
hadoop3 ansible_host=172.28.30.30 deploy_dir=/data1/bigdata/hadoop
#hadoop4 ansible_host=10.16.40.160 deploy_dir=/data1/bigdata/hadoop
#hadoop5 ansible_host=10.16.40.161 deploy_dir=/data1/bigdata/hadoop
#hadoop6 ansible_host=10.16.40.162 deploy_dir=/data1/bigdata/hadoop
#hadoop7 ansible_host=10.16.40.163 deploy_dir=/data1/bigdata/hadoop
#hadoop8 ansible_host=10.16.40.164 deploy_dir=/data1/bigdata/hadoop
#hadoop9 ansible_host=10.16.40.165 deploy_dir=/data1/bigdata/hadoop

# hdfs
[namenode_servers]
172.28.30.28
172.28.30.29

[datanode_servers]
172.28.30.28
172.28.30.29
172.28.30.30

# yarn
[resourcemanager_servers]
172.28.30.28
172.28.30.29

[nodemanager_servers]
172.28.30.28
172.28.30.29
172.28.30.30

[hive_servers]
hive1 ansible_host=172.28.30.28 deploy_dir=/data1/bigdata/hadoop
#hive2 ansible_host=10.16.40.158 deploy_dir=/data1/bigdata/hadoop

[spark_servers]
spark1 ansible_host=172.28.30.28 deploy_dir=/data1/bigdata/spark
#spark2 ansible_host=10.16.40.158 deploy_dir=/data1/bigdata/spark

[sqoop_servers]
sqoop1 ansible_host=172.28.30.28 deploy_dir=/data1/bigdata/sqoop
#sqoop2 ansible_host=10.16.40.158 deploy_dir=/data1/bigdata/sqoop

[zookeeper_servers]
zookeeper1 ansible_host=172.28.30.28 deploy_dir=/data1/bigdata/zookeeper
zookeeper2 ansible_host=172.28.30.29 deploy_dir=/data1/bigdata/zookeeper
zookeeper3 ansible_host=172.28.30.30 deploy_dir=/data1/bigdata/zookeeper

## Ds Cluster Part
[ds_api_log_alert]
ds1 ansible_host=172.28.30.28 deploy_dir=/data1/bigdata/ds

[ds_db]

[ds_masters]
ds_master1 ansible_host=172.28.30.28 deploy_dir=/data1/bigdata/ds

[ds_workers]
ds_worker1 ansible_host=172.28.30.28 deploy_dir=/data1/bigdata/ds
#ds_worker2 ansible_host=10.16.40.157 deploy_dir=/data1/bigdata/ds

## Monitoring Part
# prometheus and pushgateway servers
[monitoring_servers]
172.28.30.28

[grafana_servers]
172.28.30.28

# node_exporter and blackbox_exporter servers
[monitored_servers]
172.28.30.28
172.28.30.29
172.28.30.30

[alertmanager_servers]
172.28.30.28

[kafka_exporter_servers]

## Binlog Part
[pump_servers]

[drainer_servers]

## Group variables
[pd_servers:vars]
# location_labels = ["zone","rack","host"]

## Global variables
[all:vars]
deploy_dir = /data1/bigdata

## Connection
# ssh via normal user and port
ansible_user = tidb
ansible_port = 11822

cluster_name = test-cluster

tidb_version = v3.0.15

# don't make him special
haproxy_version = 1.5.18

java_version = 1.8.0_261
scala_version = 2.12.12

zookeeper_version = 3.4.14
hadoop_version = 2.7.7
hive_version = 2.3.7
spark_version = 2.4.4
sqoop_version = 1.4.6

ds_version = 1.3.1

# process supervision, [systemd, supervise]
process_supervision = systemd

timezone = Asia/Shanghai

enable_firewalld = False
# check NTP service
enable_ntpd = True
set_hostname = False

## binlog trigger
enable_binlog = False

# kafka cluster address for monitoring, example:
# kafka_addrs = "192.168.0.11:9092,192.168.0.12:9092,192.168.0.13:9092"
kafka_addrs = ""

# zookeeper address of kafka cluster for monitoring, example:
# zookeeper_addrs = "192.168.0.11:2181,192.168.0.12:2181,192.168.0.13:2181"
zookeeper_addrs = ""

# enable TLS authentication in the TiDB cluster
enable_tls = False

# KV mode
deploy_without_tidb = False

# wait for region replication complete before start tidb-server.
wait_replication = True

# Optional: Set if you already have a alertmanager server.
# Format: alertmanager_host:alertmanager_port
alertmanager_target = ""

grafana_admin_user = "admin"
grafana_admin_password = "admin"


### Collect diagnosis
collect_log_recent_hours = 2

enable_bandwidth_limit = True
# default: 10Mb/s, unit: Kbit/s
collect_bandwidth_limit = 10000
